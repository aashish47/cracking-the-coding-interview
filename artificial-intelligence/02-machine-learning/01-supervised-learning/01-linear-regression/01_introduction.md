# 01: Introduction to Linear Regression

Linear regression is a **supervised learning** algorithm used to predict a continuous numerical output based on one or more input features. It establishes a relationship between a **dependent variable** ($y$) and an **independent variable** ($x$) by fitting a linear equation to observed data.

## 1. The Mathematical Model

In machine learning, we represent the prediction ($\hat{y}$) as a linear function of the input $x$:

$$\hat{y} = wx + b$$

### Key Components:

- **$x$ (Feature):** The input variable or predictor (e.g., "Square Footage").
- **$w$ (Weight):** The parameter that determines the **slope**. It represents the magnitude and direction of the relationship between $x$ and $y$.
- **$b$ (Bias):** The **y-intercept**. It represents the baseline value of $\hat{y}$ when the input $x$ is zero.
- **$\hat{y}$ (Target/Label):** The predicted output value generated by the model.

## 2. Core Objective

The goal of linear regression is to find the **optimal values** for $w$ and $b$ that minimize the difference between the predicted values ($\hat{y}$) and the actual values ($y$) found in the dataset.

1. **Training:** The process of "learning" the best $w$ and $b$ using historical data.
2. **Inference:** Using the learned $w$ and $b$ to predict outcomes for new, unseen data.

## 3. Parameters vs. Hyperparameters

It is critical to distinguish between what the model learns and what we define:

| Term                         | Category           | Description                                                       |
| ---------------------------- | ------------------ | ----------------------------------------------------------------- |
| **$w$ and $b$**              | **Parameters**     | Variables the model calculates automatically during training.     |
| **Learning Rate ($\alpha$)** | **Hyperparameter** | A value set by the engineer to control how fast the model learns. |

## 4. Assumptions of the Model

For a Linear Regression model to provide reliable predictions and statistically valid inferences, certain underlying assumptions about the data must hold true. If these assumptions are violated, the model's coefficients ($w$) and predictions ($\hat{y}$) may be biased or misleading.

| Assumption                  | Definition                                                                                                    | How to Check                 | Corrective Action                      |
| :-------------------------- | :------------------------------------------------------------------------------------------------------------ | :--------------------------- | :------------------------------------- |
| **1. Linearity**            | The relationship between $x$ and $y$ must be linear; a change in $x$ results in a proportional change in $y$. | Scatter Plot                 | Transform variables (log, square root) |
| **2. Independence**         | Observations must be independent. Violated in Time-Series where values depend on previous ones.               | Durbin-Watson Test           | Use Time-Series specific models        |
| **3. Homoscedasticity**     | The variance of error terms (residuals) should be constant across all levels of $x$.                          | Residuals vs. Predicted Plot | Log transformation of $y$              |
| **4. Normality**            | Residuals should follow a normal distribution (Bell Curve) for valid hypothesis testing.                      | Q-Q Plot / Histogram         | Check for outliers or transform data   |
| **5. No Multicollinearity** | Independent variables should not be highly correlated (Multivariate only).                                    | VIF / Correlation Matrix     | Remove one of the correlated features  |

## 5. Why Use $w$ and $b$ Notation?

While classic statistics uses $\beta_0$ and $\beta_1$, the $w$ and $b$ notation is the industry standard in **Deep Learning**.

- **$w$ (Weight)** scales the input, similar to how synapses in a brain strengthen or weaken a signal.
- **$b$ (Bias)** ensures the model has the flexibility to fit data that does not pass through the origin $(0,0)$.
