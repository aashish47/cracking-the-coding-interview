# 04 Modern Contextual Embeddings

The biggest problem with Word2Vec was **Polysemy** (one word, many meanings). Modern models solve this by calculating the embedding **on the fly** based on the entire sentence.

## 1. Static vs. Dynamic: The "Bank" Example

Imagine the word **"Bank"** in two different sentences:

1. "I need to deposit money at the **bank**."
2. "The fisherman sat on the river **bank**."

| Feature               | Static (Word2Vec/GloVe)                | Dynamic (BERT/GPT)                                 |
| --------------------- | -------------------------------------- | -------------------------------------------------- |
| **Vector for "Bank"** | Exactly the same vector for both.      | Two completely different vectors.                  |
| **Calculation**       | Looked up in a pre-trained table.      | Generated by passing the sentence through a model. |
| **Meaning**           | A "confusing average" of all meanings. | Precise meaning based on neighbors.                |

---

## 2. How they are created: The Attention Mechanism

Modern embeddings are created using the **Transformer** architecture. Instead of just looking at the 2 words to the left and right, the model uses **Self-Attention**.

- **The Math:** Every word in a sentence "looks" at every other word to decide which ones are relevant.
- **The Result:** When the model creates the embedding for "bank," it sees the word "money" nearby. It uses that information to "shift" the vector for "bank" toward the **Financial** region of the vector space.

---

## 3. The Modern Training Recipe: Masked Language Modeling (MLM)

To learn these deep relationships, models like **BERT** are trained by "hiding" words and making the model guess them.

1. **Input:** "The [MASK] sat on the mat."
2. **Task:** The model must use the context ("sat", "mat") to predict that [MASK] is likely "cat" or "dog."
3. **Outcome:** Through millions of these guesses, the model learns a high-dimensional map of how human language works.

## 4. Why this matters: Transfer Learning (Again!)

Just like with CNNs, you almost never train these embeddings from scratch.

- **The Workflow:** You download a "Base" model (like `BERT-base` or `Llama-3`) that already has 1,000+ dimensions of "language knowledge."
- **The Application:** You pass your text through it to get the **Hidden States** (the embeddings), which you then use for your specific task.

## 5. Summary Table: The Evolution of Representations

| Era         | Method                      | Key Logic                      | Problem                                   |
| ----------- | --------------------------- | ------------------------------ | ----------------------------------------- |
| **Early**   | One-Hot Encoding            | Sparse IDs                     | No meaning; giant vectors.                |
| **Classic** | Word2Vec / GloVe            | Static Dense Vectors           | Cannot handle context/polysemy.           |
| **Modern**  | **Transformers (BERT/GPT)** | **Dynamic Contextual Vectors** | Computationally expensive (requires GPU). |
